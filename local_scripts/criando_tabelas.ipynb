{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c242ec35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T20:04:48.663633Z",
     "iopub.status.busy": "2022-03-04T20:04:48.662961Z",
     "iopub.status.idle": "2022-03-04T20:04:48.761889Z",
     "shell.execute_reply": "2022-03-04T20:04:48.761116Z",
     "shell.execute_reply.started": "2022-03-04T20:04:48.663445Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import sys\n",
    "from delta import *\n",
    "from re import search, sub\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType, StringType, BooleanType, FloatType, TimestampType, StructType, StructField, \\\n",
    "    DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58844717",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T20:04:48.762908Z",
     "iopub.status.busy": "2022-03-04T20:04:48.762744Z",
     "iopub.status.idle": "2022-03-04T20:04:48.767337Z",
     "shell.execute_reply": "2022-03-04T20:04:48.766179Z",
     "shell.execute_reply.started": "2022-03-04T20:04:48.762889Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append('/opt/script')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43fda3b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T20:04:48.771024Z",
     "iopub.status.busy": "2022-03-04T20:04:48.770731Z",
     "iopub.status.idle": "2022-03-04T20:04:48.782929Z",
     "shell.execute_reply": "2022-03-04T20:04:48.780997Z",
     "shell.execute_reply.started": "2022-03-04T20:04:48.770997Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = \"/home/cristianosilveira/projetos/cnes-kafka/data/BASE_DE_DADOS_CNES_202201/\"\n",
    "path_output = \"/home/cristianosilveira/projetos/cnes-kafka/data/cnes/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47a6ac2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T20:04:48.785237Z",
     "iopub.status.busy": "2022-03-04T20:04:48.784878Z",
     "iopub.status.idle": "2022-03-04T20:04:48.799756Z",
     "shell.execute_reply": "2022-03-04T20:04:48.799206Z",
     "shell.execute_reply.started": "2022-03-04T20:04:48.785201Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_files_csv_in_directory() -> list:\n",
    "    return glob.glob(f\"{path}*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5860e83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T20:04:48.800675Z",
     "iopub.status.busy": "2022-03-04T20:04:48.800476Z",
     "iopub.status.idle": "2022-03-04T20:04:48.809329Z",
     "shell.execute_reply": "2022-03-04T20:04:48.808834Z",
     "shell.execute_reply.started": "2022-03-04T20:04:48.800654Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "APP_NAME = \"teste\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df68d07f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T20:04:48.810279Z",
     "iopub.status.busy": "2022-03-04T20:04:48.810086Z",
     "iopub.status.idle": "2022-03-04T20:04:56.371403Z",
     "shell.execute_reply": "2022-03-04T20:04:56.370121Z",
     "shell.execute_reply.started": "2022-03-04T20:04:48.810258Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/04 17:04:50 WARN Utils: Your hostname, iliaDigital004 resolves to a loopback address: 127.0.1.1; using 192.168.1.12 instead (on interface wlp2s0)\n",
      "22/03/04 17:04:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark-3.1.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark-3.1.1-bin-hadoop3.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/cristianosilveira/.ivy2/cache\n",
      "The jars for the packages stored in: /home/cristianosilveira/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e58e6352-daa9-44d7-aa42-e91c2af0a0e1;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;1.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      ":: resolution report :: resolve 248ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;1.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e58e6352-daa9-44d7-aa42-e91c2af0a0e1\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/11ms)\n",
      "22/03/04 17:04:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "ANTLR Tool version 4.7 used for code generation does not match the current runtime version 4.8ANTLR Tool version 4.7 used for code generation does not match the current runtime version 4.8"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder = pyspark.sql.SparkSession.builder.appName(APP_NAME).master(\"local\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.2.21,io.delta:delta-core_2.12:1.0.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS delta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ee98286",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T20:04:56.375092Z",
     "iopub.status.busy": "2022-03-04T20:04:56.373980Z",
     "iopub.status.idle": "2022-03-04T20:04:56.397325Z",
     "shell.execute_reply": "2022-03-04T20:04:56.396171Z",
     "shell.execute_reply.started": "2022-03-04T20:04:56.374982Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_date(x): \n",
    "    try:\n",
    "        if x is not None:\n",
    "            return datetime.strptime(x, '%d/%m/%Y')\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "udf_get_date = F.udf(lambda x: get_date(x), DateType())\n",
    "\n",
    "\n",
    "def get_boolean_value(x): \n",
    "    if x and x == \"S\":\n",
    "        return True\n",
    "    return False\n",
    "udf_get_boolean_value = F.udf(lambda x: get_boolean_value(x), BooleanType())\n",
    "\n",
    "\n",
    "def get_string_value(x): \n",
    "    if x: \n",
    "        return x.replace(\"'\", \"\").replace(\",\", \";\")\n",
    "    return \"\"\n",
    "udf_get_string_value = F.udf(lambda x: get_string_value(x), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c8bbc96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T20:04:56.399278Z",
     "iopub.status.busy": "2022-03-04T20:04:56.398817Z",
     "iopub.status.idle": "2022-03-04T20:04:56.410602Z",
     "shell.execute_reply": "2022-03-04T20:04:56.409835Z",
     "shell.execute_reply.started": "2022-03-04T20:04:56.399223Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"CNES_URL\":\"jdbc:postgresql://localhost:5432/saude?zeroDateTimeBehavior=convertToNull&useTimezone=true&serverTimezone=UTC\",\n",
    "    \"CNES_USER\": \"saude\",\n",
    "    \"CNES_PASSWORD\": \"saude\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc42fd33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T20:04:56.413062Z",
     "iopub.status.busy": "2022-03-04T20:04:56.412689Z",
     "iopub.status.idle": "2022-03-04T20:05:20.387245Z",
     "shell.execute_reply": "2022-03-04T20:05:20.386363Z",
     "shell.execute_reply.started": "2022-03-04T20:04:56.413015Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TO_CHAR_REGEX = \"(?<=({}))(.*)(?=({}))\"\n",
    "list_csv = get_files_csv_in_directory()\n",
    "list_schemas = []\n",
    "for path in list_csv:\n",
    "    table_name = path.split('/')[-1]\n",
    "    table_name = table_name.split('.')[0]\n",
    "    table_name = table_name.replace('202201','')    \n",
    "    table_name = sub( '(?<!^)(?=[A-Z])', '_',table_name).lower()    \n",
    "    df = spark.read \\\n",
    "        .option(\"delimiter\", \";\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .csv(path)    \n",
    "    for column in df.columns:        \n",
    "        if \"TO_CHAR\" in column:\n",
    "            init_regex = 'TO_CHAR'\n",
    "            end_regex = ','\n",
    "            regex = search(\n",
    "                TO_CHAR_REGEX.format(init_regex, end_regex), column\n",
    "            )            \n",
    "            if regex:\n",
    "                new_column = regex[0][1:].lower()\n",
    "        else:\n",
    "            new_column = column.lower()\n",
    "        new_column = new_column.replace(\"'\", \"\")\n",
    "        df = df.withColumnRenamed(column, new_column)        \n",
    "        if new_column[0:3] in ['co_', 'nu_']:\n",
    "            df = df.withColumn(new_column, F.col(new_column).cast(IntegerType()))\n",
    "        if new_column[0:3]  == 'dt_':\n",
    "            df = df.withColumn(new_column, udf_get_date(F.col(new_column)))    \n",
    "        if new_column[0:3]  == 'st_':\n",
    "            df = df.withColumn(new_column, udf_get_boolean_value(F.col(new_column)))    \n",
    "        if new_column[0:3]  == 'ds_':\n",
    "            df = df.withColumn(new_column, udf_get_string_value(F.col(new_column)))    \n",
    "    # df \\\n",
    "    #     .repartition(1) \\\n",
    "    #     .write.format(\"csv\") \\\n",
    "    #     .mode(\"overwrite\") \\\n",
    "    #     .option(\"header\", \"true\") \\\n",
    "    #     .save(f\"{path_output}/{table_name}.csv\")\n",
    "    list_schemas.append({\"table\": table_name, \"schema\": df.schema})\n",
    "            \n",
    "#             .option(\"truncate\", \"true\") \\\n",
    "#             .option(\"url\", params[\"CNES_URL\"]) \\\n",
    "#             .option(\"user\", params[\"CNES_USER\"]) \\\n",
    "#             .option(\"password\", params[\"CNES_PASSWORD\"]) \\\n",
    "#             .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "#             .option(\"dbtable\", table_name) \\\n",
    "#             .mode(\"ignore\") \\\n",
    "#             .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a07a2c0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T20:05:20.389213Z",
     "iopub.status.busy": "2022-03-04T20:05:20.388731Z",
     "iopub.status.idle": "2022-03-04T20:05:20.414901Z",
     "shell.execute_reply": "2022-03-04T20:05:20.413745Z",
     "shell.execute_reply.started": "2022-03-04T20:05:20.389162Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_field(field_name: str, field_type: str) -> str:    \n",
    "    if field_type == \"IntegerType\":\n",
    "        return f\"{field_name} int\"\n",
    "    if field_type == \"StringType\":\n",
    "        return f\"{field_name} varchar(255)\"\n",
    "    if field_type == \"BooleanType\":\n",
    "        return f\"{field_name} boolean\"\n",
    "    if field_type == \"DateType\":\n",
    "        return f\"{field_name} timestamp\"\n",
    "\n",
    "scripts = []\n",
    "scripts_copy = []\n",
    "\n",
    "for item in list_schemas:    \n",
    "    fields = []\n",
    "    fields_copy = []\n",
    "    for field in item['schema']:\n",
    "        field_name = str(field).split('(')[1].split(',')[0]\n",
    "        field_type = str(field).split('(')[1].split(',')[1]\n",
    "        fields.append(get_field(field_name, field_type))\n",
    "        fields_copy.append(field_name)\n",
    "    sql = f\"\"\"\n",
    "            create table if not exists cnes.{item['table']} (\n",
    "                id serial not null,\n",
    "                {','.join(fields)}\n",
    "            )\n",
    "            ;            \n",
    "            ALTER TABLE cnes.{item['table']} ADD \n",
    "                CONSTRAINT PK_{item['table']} PRIMARY KEY \n",
    "                (\n",
    "                    id\n",
    "                )\n",
    "            ;            \n",
    "            \"\"\"\n",
    "    scripts.append(sql)        \n",
    "    sql = f\"\"\"insert into cnes.{item['table']} ({','.join(fields_copy)}) values (field_values);\"\"\"\n",
    "    scripts_copy.append({\"table\": item['table'], \"sql\": sql})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "373ebbda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T20:05:20.416233Z",
     "iopub.status.busy": "2022-03-04T20:05:20.415846Z",
     "iopub.status.idle": "2022-03-04T20:05:20.432424Z",
     "shell.execute_reply": "2022-03-04T20:05:20.431175Z",
     "shell.execute_reply.started": "2022-03-04T20:05:20.416202Z"
    }
   },
   "outputs": [],
   "source": [
    "# textfile = open(\"/home/cristianosilveira/projetos/cnes-kafka/postgres/cnes_schema.sql\", \"w\")\n",
    "# for element in scripts:\n",
    "#     textfile.write(element + \"\\n\")\n",
    "# textfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02247cb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T20:05:20.438206Z",
     "iopub.status.busy": "2022-03-04T20:05:20.437736Z",
     "iopub.status.idle": "2022-03-04T20:05:20.450418Z",
     "shell.execute_reply": "2022-03-04T20:05:20.449338Z",
     "shell.execute_reply.started": "2022-03-04T20:05:20.438157Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scripts_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a39b65fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T20:05:20.452839Z",
     "iopub.status.busy": "2022-03-04T20:05:20.452259Z",
     "iopub.status.idle": "2022-03-04T20:08:08.198445Z",
     "shell.execute_reply": "2022-03-04T20:08:08.195066Z",
     "shell.execute_reply.started": "2022-03-04T20:05:20.452787Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/04 17:08:06 ERROR Utils: uncaught error in thread Spark Context Cleaner, stopping SparkContext\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ContextCleaner$$Lambda$858/0x00000008406d4040.get$Lambda(Unknown Source)\n",
      "\tat java.base/java.lang.invoke.DirectMethodHandle$Holder.invokeStatic(DirectMethodHandle$Holder)\n",
      "\tat java.base/java.lang.invoke.Invokers$Holder.linkToTargetMethod(Invokers$Holder)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$1(ContextCleaner.scala:186)\n",
      "\tat org.apache.spark.ContextCleaner$$Lambda$790/0x000000084069ac40.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:180)\n",
      "\tat org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:77)\n",
      "22/03/04 17:08:06 ERROR Utils: throw uncaught fatal error in thread Spark Context Cleaner\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ContextCleaner$$Lambda$858/0x00000008406d4040.get$Lambda(Unknown Source)\n",
      "\tat java.base/java.lang.invoke.DirectMethodHandle$Holder.invokeStatic(DirectMethodHandle$Holder)\n",
      "\tat java.base/java.lang.invoke.Invokers$Holder.linkToTargetMethod(Invokers$Holder)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$1(ContextCleaner.scala:186)\n",
      "\tat org.apache.spark.ContextCleaner$$Lambda$790/0x000000084069ac40.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:180)\n",
      "\tat org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:77)\n",
      "Exception in thread \"Spark Context Cleaner\" java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ContextCleaner$$Lambda$858/0x00000008406d4040.get$Lambda(Unknown Source)\n",
      "\tat java.base/java.lang.invoke.DirectMethodHandle$Holder.invokeStatic(DirectMethodHandle$Holder)\n",
      "\tat java.base/java.lang.invoke.Invokers$Holder.linkToTargetMethod(Invokers$Holder)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$1(ContextCleaner.scala:186)\n",
      "\tat org.apache.spark.ContextCleaner$$Lambda$790/0x000000084069ac40.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:180)\n",
      "\tat org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:77)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o11355.collectToPython.\n: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.next(SparkPlan.scala:373)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.next(SparkPlan.scala:369)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.foreach(SparkPlan.scala:369)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1(SparkPlan.scala:391)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1$adapted(SparkPlan.scala:390)\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2688/0x000000084118e840.apply(Unknown Source)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3519)\n\tat org.apache.spark.sql.Dataset$$Lambda$2642/0x0000000841167840.apply(Unknown Source)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.Dataset$$Lambda$1374/0x0000000840bacc40.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1382/0x0000000840bb9040.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1375/0x0000000840bad840.apply(Unknown Source)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3516)\n\tat jdk.internal.reflect.GeneratedMethodAccessor58.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m table \u001b[38;5;241m=\u001b[39m [d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m scripts_copy \u001b[38;5;28;01mif\u001b[39;00m d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m table_name][\u001b[38;5;241m0\u001b[39m]        \n\u001b[1;32m      6\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread \\\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39mcsv(file_name)\n\u001b[0;32m---> 10\u001b[0m records \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/cristianosilveira/projetos/cnes-kafka/postgres/data/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.sql\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m records:            \n",
      "File \u001b[0;32m~/projetos/cnes-kafka/local_scripts/.env/lib/python3.9/site-packages/pyspark/sql/dataframe.py:693\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \n\u001b[1;32m    685\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;124;03m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc) \u001b[38;5;28;01mas\u001b[39;00m css:\n\u001b[0;32m--> 693\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))\n",
      "File \u001b[0;32m~/projetos/cnes-kafka/local_scripts/.env/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/projetos/cnes-kafka/local_scripts/.env/lib/python3.9/site-packages/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/projetos/cnes-kafka/local_scripts/.env/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o11355.collectToPython.\n: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.next(SparkPlan.scala:373)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.next(SparkPlan.scala:369)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.foreach(SparkPlan.scala:369)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1(SparkPlan.scala:391)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1$adapted(SparkPlan.scala:390)\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2688/0x000000084118e840.apply(Unknown Source)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3519)\n\tat org.apache.spark.sql.Dataset$$Lambda$2642/0x0000000841167840.apply(Unknown Source)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.Dataset$$Lambda$1374/0x0000000840bacc40.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1382/0x0000000840bb9040.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1375/0x0000000840bad840.apply(Unknown Source)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3516)\n\tat jdk.internal.reflect.GeneratedMethodAccessor58.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n"
     ]
    }
   ],
   "source": [
    "path_data = \"/home/cristianosilveira/projetos/cnes-kafka/data/cnes/\"\n",
    "\n",
    "for file_name in glob.glob(f\"{path_data}*.csv\"):\n",
    "    table_name = file_name.split('/')[-1].split('.')[0]        \n",
    "    table = [d for d in scripts_copy if d['table'] == table_name][0]        \n",
    "    df = spark.read \\\n",
    "        .option(\"delimiter\", \";\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .csv(file_name)\n",
    "    records = df.collect()\n",
    "    with open(f\"/home/cristianosilveira/projetos/cnes-kafka/postgres/data/{table['table']}.sql\", \"w\") as f:\n",
    "        for record in records:            \n",
    "            field_values = str(record[0]).replace('\"\"', 'null').split(',')\n",
    "            field_values = \"'\" + \"','\".join(field_values).replace(\"'null'\", 'null') + \"'\"         \n",
    "            element = table['sql'].replace(\"field_values\", field_values).replace(\"D' \", 'D').replace(\"'null'\", 'null')\n",
    "            f.write(element + \"\\n\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ffdb00",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-03-04T20:08:08.200171Z",
     "iopub.status.idle": "2022-03-04T20:08:08.200827Z",
     "shell.execute_reply": "2022-03-04T20:08:08.200583Z",
     "shell.execute_reply.started": "2022-03-04T20:08:08.200552Z"
    }
   },
   "outputs": [],
   "source": [
    "# [d for d in scripts_copy if d['table'] == 'rl_estab_equipe_prof']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de8181c",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-03-04T20:08:08.202203Z",
     "iopub.status.idle": "2022-03-04T20:08:08.203069Z",
     "shell.execute_reply": "2022-03-04T20:08:08.202831Z",
     "shell.execute_reply.started": "2022-03-04T20:08:08.202798Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# textfile = open(\"/home/cristianosilveira/projetos/cnes-kafka/postgres/cnes_script.sql\", \"w\")\n",
    "# for element in inserts:\n",
    "#     textfile.write(element + \"\\n\")\n",
    "# textfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
